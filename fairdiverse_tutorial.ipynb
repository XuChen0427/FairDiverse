{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import fairdiverse\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/clararus/PycharmProjects/FairDiverse/FairDiverse/fairdiverse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clararus/anaconda3/envs/fair_diverse/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd fairdiverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(model_name, dataset_name):\n",
    "    if dataset_name !=\"\":\n",
    "        today = date.today()\n",
    "        today_format = f\"{today.year}-{today.month}-{today.day}\"\n",
    "        \n",
    "        # read evaluation file\n",
    "        evaluation_file = f\"recommendation/log/{today_format}_{model_name}_{dataset_name}/test_result.json\"\n",
    "       \n",
    "    else:\n",
    "        evaluation_file = f\"recommendation/log/{model_name}/test_result.json\"\n",
    "\n",
    "    with open(evaluation_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        metrics = json.load(f)\n",
    "    # format metrics as table for visualisation\n",
    "    table = {}\n",
    "    for metric_key, value in metrics.items():\n",
    "        metric, k = metric_key.split(\"@\")\n",
    "        if metric not in table:\n",
    "            table[metric] = {}\n",
    "        table[metric][f\"@{k}\"] = value\n",
    "    \n",
    "    df = pd.DataFrame(table).T\n",
    "    df = df[sorted(df.columns, key=lambda x: int(x[1:]))]\n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0yFRo1djsdS"
   },
   "source": [
    "# üß∞ FairDiverse Tutorial\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **1. Add New Dataset üìÅ**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "### Step 1: ‚¨áÔ∏è Download the Dataset from RecBole\n",
    "The RecBole project maintains a collection of ready-to-use datasets: [RecBole Datasets GitHub](https://github.com/RUCAIBox/RecSysDatasets)\n",
    "\n",
    "Navigate to:  **Google Drive ‚Üí Processed Datasets ‚Üí Choose a dataset (e.g., movielens-1m.zip)**\n",
    "\n",
    "#### What if the Dataset is Not in RecBole Format?\n",
    "Follow the steps [here](https://recbole.io/docs/user_guide/usage/running_new_dataset.html) in order to convert your data files to RecBole format which uses atomic files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "\n",
    "#### üé¨ MovieLens Dataset\n",
    "In this notebook we will use the MovieLens Dataset as an example.\n",
    "\n",
    "GroupLens Research has collected and made available rating data sets from the MovieLens web site (https://movielens.org). This dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service.\n",
    "\n",
    "**Download the MovieLens dataset from RecBole: [MovieLens Dataset (RecBole processed)](https://drive.google.com/file/d/1G7_XhdSi1BhIvRETg0nN0O5tuOvbEs65/view?usp=drive_link)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ml-100k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 2:** Place the dataset files under `~/recommendation/dataset/ml-100k`\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "        ‚îî‚îÄ‚îÄ dataset\n",
    "            ‚îî‚îÄ‚îÄ ml-100k\n",
    "                ‚îú‚îÄ‚îÄ ml-100k.inter\n",
    "                ‚îú‚îÄ‚îÄ ml-100k.item\n",
    "                ‚îú‚îÄ‚îÄ ml-100k.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"recommendation/dataset/{dataset_name}\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "# move the dataset files in the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### Dataset Content üìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "---\n",
    "**User Data**\n",
    "\n",
    "The file ml-100k.user comprising the attributes of the users.\n",
    "\n",
    "Each record/line in the file has the following fields: \n",
    " \n",
    "- `user_id`: the id of the users and its type is token.\n",
    "- `age`: the age of the users, and its type is float.\n",
    "- `gender`: the gender of the users, and its type is token.\n",
    "- `occupation`: the occupation of the users, and its type is token.\n",
    "- `zip_code`: the zip_code of the users, and its type is token.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample\n",
      "   user_id:token  age:token gender:token occupation:token zip_code:token\n",
      "0              1         24            M       technician          85711\n",
      "1              2         53            F            other          94043\n",
      "2              3         23            M           writer          32067\n",
      "3              4         24            M       technician          43537\n",
      "4              5         33            F            other          15213\n"
     ]
    }
   ],
   "source": [
    "user_path = os.path.join(data_path, \"ml-100k.user\")\n",
    "user_data = pd.read_csv(user_path,delimiter='\\t')\n",
    "\n",
    "print(f\"Data Sample\")\n",
    "print(user_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Users: 943\n"
     ]
    }
   ],
   "source": [
    "num_users = user_data[\"user_id:token\"].nunique()\n",
    "print(f\"Total Users: {num_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "---\n",
    "**Item Data**\n",
    "\n",
    "The file ml-100k.item comprising the attributes of the movies.\n",
    "\n",
    "Each record/line in the file has the following fields: \n",
    " \n",
    "- `item_id`: the id of the movies and its type is token.\n",
    "- `movie_title`: the title of the movies, and its type is token_seq.\n",
    "- `release_year`: the year when movies were released, and its type is float.\n",
    "- `class`: the classes (genres) of the movies, and its type is token_seq.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   item_id:token movie_title:token_seq release_year:token  \\\n",
      "0              1             Toy Story               1995   \n",
      "1              2             GoldenEye               1995   \n",
      "2              3            Four Rooms               1995   \n",
      "3              4            Get Shorty               1995   \n",
      "4              5               Copycat               1995   \n",
      "\n",
      "               class:token_seq  \n",
      "0  Animation Children's Comedy  \n",
      "1    Action Adventure Thriller  \n",
      "2                     Thriller  \n",
      "3          Action Comedy Drama  \n",
      "4         Crime Drama Thriller  \n"
     ]
    }
   ],
   "source": [
    "item_path = os.path.join(data_path, \"ml-100k.item\")\n",
    "item_data = pd.read_csv(item_path,delimiter='\\t')\n",
    "\n",
    "print(item_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Items: 1682\n"
     ]
    }
   ],
   "source": [
    "num_items = item_data[\"item_id:token\"].nunique()\n",
    "print(f\"Total Items: {num_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "\n",
    "##### For simplicity we consider only the first class for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data['first_class:token'] = item_data['class:token_seq'].apply(lambda x: x.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_class:token</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Animation</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children's</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Crime</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Documentary</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Drama</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Film-Noir</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Horror</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Musical</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Romance</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>War</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Western</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>unknown</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_class:token    0\n",
       "0             Action  251\n",
       "1          Adventure   60\n",
       "2          Animation   35\n",
       "3         Children's   48\n",
       "4             Comedy  426\n",
       "5              Crime   74\n",
       "6        Documentary   49\n",
       "7              Drama  531\n",
       "8            Fantasy    1\n",
       "9          Film-Noir   14\n",
       "10            Horror   62\n",
       "11           Musical    9\n",
       "12           Mystery   20\n",
       "13           Romance   31\n",
       "14            Sci-Fi   15\n",
       "15          Thriller   36\n",
       "16               War    5\n",
       "17           Western   13\n",
       "18           unknown    2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of movie classes\n",
    "item_data.groupby('first_class:token').size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove items with unknown class\n",
    "item_data = item_data[item_data['first_class:token'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save item_data\n",
    "item_data.to_csv(item_path,sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "---\n",
    "**Interaction Data**\n",
    "The file ml-100k.inter comprising the ratings of users over the movies.\n",
    "\n",
    "Each record/line in the file has the following fields: \n",
    "\n",
    "- `user_id`: the id of the users and its type is token. \n",
    "- `item_id`: the id of the movies and its type is token.\n",
    "- `rating`: the rating of the users over the movies, and its type is float with values in {1, 2, 3, 4, 5}.\n",
    "- `timestamp`: the UNIX timestamp of the rating, and its type is float.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id:token  item_id:token  rating:float  timestamp:float\n",
      "0            196            242             3        881250949\n",
      "1            186            302             3        891717742\n",
      "2             22            377             1        878887116\n",
      "3            244             51             2        880606923\n",
      "4            166            346             1        886397596\n"
     ]
    }
   ],
   "source": [
    "interaction_path = os.path.join(data_path, \"ml-100k.inter\")\n",
    "interaction_data = pd.read_csv(interaction_path,delimiter='\\t')\n",
    "\n",
    "# remove from interaction data items which were dropped\n",
    "interaction_data = interaction_data[interaction_data['item_id:token'].isin(item_data['item_id:token'])]\n",
    "\n",
    "print(interaction_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items interacted with:  1680\n",
      "Number of users who performed an interaction:  943\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of items interacted with: \", len(interaction_data[\"item_id:token\"].unique()))\n",
    "print(\"Number of users who performed an interaction: \", len(interaction_data[\"user_id:token\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating:float</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>27142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>34170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>21200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating:float      0\n",
       "0             1   6108\n",
       "1             2  11370\n",
       "2             3  27142\n",
       "3             4  34170\n",
       "4             5  21200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distirbution of movie ratings\n",
    "interaction_data.groupby(\"rating:float\").size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save interaction_data\n",
    "interaction_data.to_csv(interaction_path,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 3:** Create a configuration file for the dataset under `~/recommendation/properties/dataset/ml-100k.yaml`\n",
    "\n",
    "```yaml\n",
    "{\n",
    "    user_id: user_id:token, # column name of the user ID\n",
    "    item_id: item_id:token, # column name of the item ID, in this case we recommend movies\n",
    "    group_id: first_class:token, # column name of the groups to be considered for fairness, in this case we consider the genres of the movie\n",
    "    label_id: rating:float, # column name for the label, indicating the interest of the user in the item\n",
    "    timestamp: timestamp:float, # column name for the timestamp of when the interaction happened\n",
    "    text_id: movie_title:token_seq, # column name for the text ID of the item (e.g. movie name, book title)\n",
    "    label_threshold: 3, # if label exceed the value will be regarded as 1, otherwise, it will be accounted into 0 --> we consider a positive recommendation if a user rated a movie with a value higher than 3\n",
    "    item_domain: movie, # description of the dataset domain (e.g. movie, music, jobs etc.)\n",
    "\n",
    "   item_val: 5, # keep items which have at least this number of interactions\n",
    "   user_val: 5, # keep users who have at least this number of interactions\n",
    "   group_val: 5, # keep groups which have at least this number of interactions\n",
    "   group_aggregation_threshold: 15, ##If the number of items owned by a group is less than this value, those groups will be merged into a single group called the 'infrequent group'. For example, Fantasy, War, Musician, ... will be merged into one group called 'infrequent group', as the number of items belonging to this group is under the threshold.\n",
    "   sample_size: 1.0, ###Sample ratio of the whole dataset to form a new subset dataset for training.\n",
    "   valid_ratio: 0.1, ### Samples to be used for validation\n",
    "   test_ratio: 0.1, ### Samples to be used for test\n",
    "   reprocess: True, ##do you need to re-process the dataset according to your personalized requirements\n",
    "   sample_num: 300, # needs to be higher than the max number of positive samples per user\n",
    "   history_length: 20, # length of historical interactions of a user - [item_1, item_2, item_3, ...] to be considered\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"user_id\": \"user_id:token\",  \n",
    "    \"item_id\": \"item_id:token\",  \n",
    "    \"group_id\": \"first_class:token\",  \n",
    "    \"label_id\": \"rating:float\",  \n",
    "    \"timestamp\": \"timestamp:float\",  \n",
    "    \"text_id\": \"movie_title:token_seq\",  \n",
    "    \"label_threshold\": 3,  \n",
    "    \"item_domain\": \"movie\",\n",
    "\n",
    "\n",
    "    \"item_val\": 5, \n",
    "    \"user_val\": 5, \n",
    "    \"group_val\": 5, \n",
    "    \"group_aggregation_threshold\": 15, \n",
    "    \"sample_size\": 1.0, \n",
    "    \"valid_ratio\": 0.1, \n",
    "    \"test_ratio\": 0.1, \n",
    "    \"reprocess\": True, \n",
    "    \"sample_num\": 350, \n",
    "    \"history_length\": 20, \n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/properties/dataset/{dataset_name}.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_data, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataset as a choice in main.py\n",
    "with open(\"main.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "content = content.replace(\"choices=[\\\"steam\\\", \\\"clueweb09\\\", \\\"compas\\\"]\", f\"choices=[\\\"steam\\\", \\\"clueweb09\\\", \\\"compas\\\", \\\"{dataset_name}\\\"]\")\n",
    "with open(\"main.py\", \"w\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **2. Base Recommender System**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "To check that the set-up of the new dataset works well, let's train a base recommender system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Define your training configuration file: `~/recommendation/train-base-model.yaml`\n",
    "\n",
    "You can change parameters specific to each model in the following configuration file: `recommendation/properties/models/<model_name>.yaml`\n",
    "\n",
    "```yaml\n",
    "{\n",
    "   ############base model#########################\n",
    "   model: SASRec, # define the model to train\n",
    "   data_type: 'sequential', #[point, pair, sequential] # define the data_type needed by the model during training SASRec is a sequnetial recommender system, expecting the data_type to be 'sequential'\n",
    "   #############################################################\n",
    "\n",
    "   ##Should the preprocessing be redone based on the new parameters instead of using the cached files in ~/recommendation/process_dataset######\n",
    "   reprocess: True,\n",
    "   ###############################################\n",
    "\n",
    "  ####fair-rank model settings --> set all to False as we want to only train the base model without any fairness/diversity intervention\n",
    "   fair-rank: False, ##if you want to run a fair-rank module on the base models, you should set the value as True\n",
    "\n",
    "  # LLM recommendation setting\n",
    "   use_llm: False,\n",
    "\n",
    "  #############log name, it will store the evaluation result in ~log/your_log_name/\n",
    "   log_name: \"SASRec_ml-100k\",\n",
    "  #################################################\n",
    "\n",
    "   ###########################training parameters################################\n",
    "   device: cpu,\n",
    "   epoch: 20,\n",
    "   batch_size: 64,\n",
    "   learning_rate: 0.001,\n",
    "   ###########################################################################\n",
    "\n",
    "\n",
    "   ###################################evaluation parameters: overwrite from ~/properties/evaluation.yaml######################################\n",
    "   mmf_eval_ratio: 0.5,\n",
    "   decimals: 4,\n",
    "   eval_step: 5,\n",
    "   eval_type: 'ranking',\n",
    "   watch_metric: 'mmf@20',\n",
    "   topk: [ 5,10,20 ], # if you choose the ranking settings, you can choose your top-k list\n",
    "   store_scores: True, #If set true, the all relevance scores will be stored in the ~/log/your_name/ for post-processing\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   ###########################################################################\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with the baselines models provided by FairDiverse\n",
    "base_model_name = \"SASRec\"\n",
    "config_base = {\n",
    "    # ############ base model #########################\n",
    "    \"model\": f\"{base_model_name}\",  \n",
    "    \"data_type\": \"sequential\",  \n",
    "\n",
    "    # Should preprocessing be redone (ignore cache)?\n",
    "    \"reprocess\": True,\n",
    "\n",
    "    # Fair-rank settings !!! Don't change - needs to be set to False for running the base model !!!\n",
    "    \"fair-rank\": False,  # run fair-rank module or not\n",
    "\n",
    "    # LLM recommendation setting !!! Don't change - needs to be set to False for running the base model !!!\n",
    "    \"use_llm\": False,\n",
    "\n",
    "    # Log name (results will be stored in ~/log/{log_name}/)\n",
    "    \"log_name\": f\"{base_model_name}_{dataset_name}\",\n",
    "\n",
    "    # ############# training parameters #################\n",
    "    \"device\": \"cpu\",\n",
    "    \"epoch\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    # ############# evaluation parameters #################\n",
    "    \"mmf_eval_ratio\": 0.5,\n",
    "    \"decimals\": 4,\n",
    "    \"eval_step\": 5,\n",
    "    \"eval_type\": \"ranking\",\n",
    "    \"watch_metric\": \"mmf@20\",\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"store_scores\": True,\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # [\"Exposure\", \"Utility\"]\n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/train-base-model.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_base, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 2: Run the Base Recommender System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your training config...\n",
      "{'model': 'SASRec', 'data_type': 'sequential', 'reprocess': True, 'fair-rank': False, 'use_llm': False, 'log_name': 'SASRec_ml-100k', 'device': 'cpu', 'epoch': 20, 'batch_size': 64, 'learning_rate': 0.001, 'mmf_eval_ratio': 0.5, 'decimals': 4, 'eval_step': 5, 'eval_type': 'ranking', 'watch_metric': 'mmf@20', 'topk': [5, 10, 20], 'store_scores': True, 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'ml-100k', 'stage': 'in-processing', 'task': 'recommendation'}\n",
      "your args: Namespace(task='recommendation', stage='in-processing', dataset='ml-100k', train_config_file='train-base-model.yaml')\n",
      "process config:\n",
      "{'item_val': 5, 'user_val': 5, 'group_val': 5, 'group_aggregation_threshold': 15, 'sample_size': 1.0, 'valid_ratio': 0.1, 'test_ratio': 0.1, 'reprocess': True, 'sample_num': 350, 'history_length': 20, 'user_id': 'user_id:token', 'item_id': 'item_id:token', 'group_id': 'first_class:token', 'label_id': 'rating:float', 'timestamp': 'timestamp:float', 'text_id': 'movie_title:token_seq', 'label_threshold': 3, 'item_domain': 'movie', 'model': 'SASRec', 'data_type': 'sequential', 'fair-rank': False, 'use_llm': False, 'log_name': 'SASRec_ml-100k', 'device': 'cpu', 'epoch': 20, 'batch_size': 64, 'learning_rate': 0.001, 'mmf_eval_ratio': 0.5, 'decimals': 4, 'eval_step': 5, 'eval_type': 'ranking', 'watch_metric': 'mmf@20', 'topk': [5, 10, 20], 'store_scores': True, 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'ml-100k', 'stage': 'in-processing', 'task': 'recommendation'}\n",
      "start to process data...\n",
      "1680\n",
      "origin:----------item number: 1680 user number:943 group_num:18 total interactions:99990\n",
      "start to merge data.....\n",
      "processing item val\n",
      "item number: 1345 user number:822 group number:14 total interactions:76997\n",
      "start to sample and split data....\n",
      "checking.....\n",
      "min user interactions: 1\n",
      "min item interactions: 1\n",
      "/Users/clararus/PycharmProjects/FairDiverse/FairDiverse/fairdiverse/recommendation/process_dataset.py:264: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_frame.rename(columns={\n",
      "/Users/clararus/PycharmProjects/FairDiverse/FairDiverse/fairdiverse/recommendation/process_dataset.py:269: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_frame.sort_values(by=time_field, inplace=True)\n",
      "       user_id:token  item_id:token  ... first_class:token  timestamp:float\n",
      "12799            774            891  ...                 0      891385190.0\n",
      "17608            774           1012  ...                 1      891385190.0\n",
      "7005             774             77  ...                10      891385190.0\n",
      "68665            774            990  ...                 6      891385190.0\n",
      "35372            541             34  ...                 2      891385197.0\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "start to construct ranking and retrieval test dataset...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7699/7699 [00:00<00:00, 171937.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:00<00:00, 779.42it/s]\n",
      "max_len:294\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7701/7701 [00:00<00:00, 175146.46it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 745.47it/s]\n",
      "max_len:319\n",
      "start to load config...\n",
      "start to load model...\n",
      "your loading config is:\n",
      "{'batch_size': 64, 'data_type': 'sequential', 'dataset': 'ml-100k', 'decimals': 4, 'device': 'cpu', 'epoch': 20, 'eval_step': 5, 'eval_type': 'ranking', 'fair-rank': False, 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'group_aggregation_threshold': 15, 'group_id': 'first_class:token', 'group_num': 14, 'group_val': 5, 'history_field': 'history_behaviors', 'history_length': 20, 'item_domain': 'movie', 'item_id': 'item_id:token', 'item_num': 1345, 'item_val': 5, 'label_id': 'label:float', 'label_threshold': 3, 'learning_rate': 0.001, 'log_name': 'SASRec_ml-100k', 'mmf_eval_ratio': 0.5, 'model': 'SASRec', 'reprocess': True, 'sample_num': 350, 'sample_size': 1.0, 'stage': 'in-processing', 'store_scores': True, 'task': 'recommendation', 'test_ratio': 0.1, 'text_id': 'movie_title:token_seq', 'timestamp': 'timestamp:float', 'topk': [5, 10, 20], 'use_llm': False, 'user_id': 'user_id:token', 'user_num': 822, 'user_val': 5, 'valid_ratio': 0.1, 'watch_metric': 'mmf@20', 'n_layers': 1, 'n_heads': 2, 'embedding_size': 32, 'inner_size': 64, 'hidden_dropout_prob': 0.5, 'attn_dropout_prob': 0.5, 'hidden_act': 'gelu', 'layer_norm_eps': '1e-12', 'initializer_range': 0.02, 'eval_batch_size': 128}\n",
      "process complete! The file and config are stored in recommendation/processed_dataset/ml-100k\n",
      "start to load dataset......\n",
      "   user_id  ... pos_length\n",
      "0        4  ...         15\n",
      "1       10  ...        138\n",
      "2       13  ...          3\n",
      "3       17  ...          3\n",
      "4       20  ...          2\n",
      "\n",
      "[5 rows x 4 columns]\n",
      "start to train...\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 24.00it/s]\u001b[A\n",
      "eval result: {'ndcg@5': np.float64(0.2926), 'ndcg@10': np.float64(0.2923), 'ndcg@20': np.float64(0.3084), 'mrr@5': np.float64(0.3783), 'mrr@10': np.float64(0.392), 'mrr@20': np.float64(0.4022), 'hr@5': np.float64(0.3009), 'hr@10': np.float64(0.3093), 'hr@20': np.float64(0.366), 'mmf@5': np.float64(0.0573), 'mmf@10': np.float64(0.0691), 'mmf@20': np.float64(0.0885), 'gini@5': np.float64(0.6899), 'gini@10': np.float64(0.6705), 'gini@20': np.float64(0.6386), 'entropy@5': np.float64(2.5092), 'entropy@10': np.float64(2.5805), 'entropy@20': np.float64(2.6794), 'maxminratio@5': np.float64(0.0), 'maxminratio@10': np.float64(0.0064), 'maxminratio@20': np.float64(0.006)}, best result: 0.0885\n",
      "\n",
      "epoch: 0 loss: 6.555\n",
      "  5%|‚ñà‚ñà‚ñè                                         | 1/20 [00:04<01:24,  4.46s/it]epoch: 1 loss: 6.198\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 2/20 [00:08<01:14,  4.15s/it]epoch: 2 loss: 6.074\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 3/20 [00:12<01:08,  4.01s/it]epoch: 3 loss: 6.028\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 4/20 [00:16<01:02,  3.93s/it]epoch: 4 loss: 5.995\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 5/20 [00:20<01:01,  4.09s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 30.97it/s]\u001b[A\n",
      "eval result: {'ndcg@5': np.float64(0.3847), 'ndcg@10': np.float64(0.375), 'ndcg@20': np.float64(0.3869), 'mrr@5': np.float64(0.498), 'mrr@10': np.float64(0.5079), 'mrr@20': np.float64(0.5154), 'hr@5': np.float64(0.3888), 'hr@10': np.float64(0.3837), 'hr@20': np.float64(0.436), 'mmf@5': np.float64(0.0539), 'mmf@10': np.float64(0.082), 'mmf@20': np.float64(0.0955), 'gini@5': np.float64(0.6682), 'gini@10': np.float64(0.6355), 'gini@20': np.float64(0.6269), 'entropy@5': np.float64(2.6073), 'entropy@10': np.float64(2.7185), 'entropy@20': np.float64(2.7365), 'maxminratio@5': np.float64(0.0037), 'maxminratio@10': np.float64(0.006), 'maxminratio@20': np.float64(0.0105)}, best result: 0.0955\n",
      "\n",
      "epoch: 5 loss: 5.971\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 6/20 [00:24<00:56,  4.04s/it]epoch: 6 loss: 5.956\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 7/20 [00:28<00:51,  3.96s/it]epoch: 7 loss: 5.942\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 8/20 [00:31<00:46,  3.91s/it]epoch: 8 loss: 5.932\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 9/20 [00:35<00:42,  3.89s/it]epoch: 9 loss: 5.918\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 10/20 [00:39<00:38,  3.86s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 31.42it/s]\u001b[A\n",
      "eval result: {'ndcg@5': np.float64(0.3869), 'ndcg@10': np.float64(0.3893), 'ndcg@20': np.float64(0.3988), 'mrr@5': np.float64(0.4962), 'mrr@10': np.float64(0.5056), 'mrr@20': np.float64(0.5119), 'hr@5': np.float64(0.3959), 'hr@10': np.float64(0.4129), 'hr@20': np.float64(0.4596), 'mmf@5': np.float64(0.0663), 'mmf@10': np.float64(0.0775), 'mmf@20': np.float64(0.0941), 'gini@5': np.float64(0.6629), 'gini@10': np.float64(0.6502), 'gini@20': np.float64(0.623), 'entropy@5': np.float64(2.6236), 'entropy@10': np.float64(2.666), 'entropy@20': np.float64(2.7553), 'maxminratio@5': np.float64(0.0036), 'maxminratio@10': np.float64(0.0053), 'maxminratio@20': np.float64(0.0164)}, best result: 0.0941\n",
      "\n",
      "epoch: 10 loss: 5.907\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 11/20 [00:43<00:34,  3.86s/it]epoch: 11 loss: 5.900\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 12/20 [00:47<00:30,  3.84s/it]epoch: 12 loss: 5.896\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 13/20 [00:51<00:26,  3.83s/it]epoch: 13 loss: 5.886\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 14/20 [00:54<00:22,  3.83s/it]epoch: 14 loss: 5.885\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 15/20 [00:58<00:19,  3.82s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 31.61it/s]\u001b[A\n",
      "eval result: {'ndcg@5': np.float64(0.4044), 'ndcg@10': np.float64(0.3988), 'ndcg@20': np.float64(0.4074), 'mrr@5': np.float64(0.5141), 'mrr@10': np.float64(0.5228), 'mrr@20': np.float64(0.5293), 'hr@5': np.float64(0.4136), 'hr@10': np.float64(0.4184), 'hr@20': np.float64(0.4567), 'mmf@5': np.float64(0.0775), 'mmf@10': np.float64(0.0865), 'mmf@20': np.float64(0.093), 'gini@5': np.float64(0.657), 'gini@10': np.float64(0.6345), 'gini@20': np.float64(0.6273), 'entropy@5': np.float64(2.6317), 'entropy@10': np.float64(2.7238), 'entropy@20': np.float64(2.7347), 'maxminratio@5': np.float64(0.0), 'maxminratio@10': np.float64(0.0091), 'maxminratio@20': np.float64(0.0182)}, best result: 0.093\n",
      "\n",
      "epoch: 15 loss: 5.876\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 16/20 [01:02<00:15,  3.83s/it]epoch: 16 loss: 5.872\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 17/20 [01:06<00:11,  3.83s/it]epoch: 17 loss: 5.868\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 18/20 [01:10<00:07,  3.82s/it]epoch: 18 loss: 5.862\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 19/20 [01:13<00:03,  3.82s/it]epoch: 19 loss: 5.857\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [01:17<00:00,  3.89s/it]\n",
      "training complete! start to save the config and model...\n",
      " config files are dump in recommendation/log/2025-5-30_SASRec_ml-100k\n",
      "start to testing...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 36.67it/s]\n",
      "{'ndcg@5': np.float64(0.3337), 'ndcg@10': np.float64(0.352), 'ndcg@20': np.float64(0.3584), 'mrr@5': np.float64(0.4371), 'mrr@10': np.float64(0.4495), 'mrr@20': np.float64(0.4538), 'hr@5': np.float64(0.339), 'hr@10': np.float64(0.3823), 'hr@20': np.float64(0.41), 'mmf@5': np.float64(0.0648), 'mmf@10': np.float64(0.0724), 'mmf@20': np.float64(0.089), 'gini@5': np.float64(0.6563), 'gini@10': np.float64(0.6403), 'gini@20': np.float64(0.6193), 'entropy@5': np.float64(2.6317), 'entropy@10': np.float64(2.6748), 'entropy@20': np.float64(2.7412), 'maxminratio@5': np.float64(0.0046), 'maxminratio@10': np.float64(0.005), 'maxminratio@20': np.float64(0.0223)}\n",
      "dump in recommendation/log/2025-5-30_SASRec_ml-100k\n"
     ]
    }
   ],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"in-processing\" --dataset \"{dataset_name}\" --train_config_file \"train-base-model.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Output files**\n",
    "---\n",
    "\n",
    "**Processed Dataset Structure**\n",
    "\n",
    "The following files are generated during preprocessing and saved under `processed_dataset/ml-100k/`:\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ iid2pid.json              # Mapping from item ID to provider/group ID\n",
    "            ‚îú‚îÄ‚îÄ iid2text.json             # Mapping from item ID to textual representation (e.g., title)\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.test.CTR       # Test set for click-through rate (CTR) evaluation\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.test.ranking   # Test set for ranking evaluation\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.train          # Training set\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.valid.CTR      # Validation set for CTR evaluation\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.valid.ranking  # Validation set for ranking evaluation\n",
    "            ‚îî‚îÄ‚îÄ process_config.yaml       # Configuration used during preprocessing\n",
    "```\n",
    "**Log Output Directory Structure**\n",
    "\n",
    "After training, the following files are saved under the `log/` directory:\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄlog/\n",
    "        ‚îî‚îÄ‚îÄ 2025-5-20_SASRec_ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ best_model.pth         # Saved PyTorch model weights\n",
    "            ‚îú‚îÄ‚îÄ config.yaml            # Configuration used for training\n",
    "            ‚îú‚îÄ‚îÄ ranking_scores.npz     # Numpy array of ranking scores\n",
    "            ‚îî‚îÄ‚îÄ test_result.json       # Evaluation metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Evaluation Results üìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         0.3337  0.3520  0.3584\n",
      "mrr          0.4371  0.4495  0.4538\n",
      "hr           0.3390  0.3823  0.4100\n",
      "mmf          0.0648  0.0724  0.0890\n",
      "gini         0.6563  0.6403  0.6193\n",
      "entropy      2.6317  2.6748  2.7412\n",
      "maxminratio  0.0046  0.0050  0.0223\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **3. Run Post-processing Model**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **3.1 With Input from FairDiverse**\n",
    "\n",
    "---\n",
    "\n",
    "Run the post-processing model on-top of the base recommender system that we have trained in Section 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Create a configuration file for running a post-processing intervention under \n",
    "You can change parameters specific to each model in the following configuration file: `recommendation/properties/models/<model_name>.yaml` \n",
    "```yaml\n",
    "{\n",
    "   ###############the ranking score stored path for the post-processing##################\n",
    "   ranking_store_path: \"ml-100k\", \n",
    "   #######################################################################################\n",
    "\n",
    "   ### !!! Don't change - needs to be set to False as we don't run a post-processing intervention !!!\n",
    "   model: \"CPFair\",\n",
    "   log_name: \"CPFair_ml-100k\",\n",
    "\n",
    "   #########################Evaluation parameters#########################################\n",
    "   topk: [5, 10, 20],\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   #####################################################################################\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_model_name = \"CPFair\"\n",
    "today = date.today()\n",
    "today_format = f\"{today.year}-{today.month}-{today.day}\"\n",
    "    \n",
    "config_model = {\n",
    "    \"ranking_store_path\": f\"{today_format}_{base_model_name}_{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Change to any of the supported post-processing methods in Fairdiverse\n",
    "    \"model\": f\"{postprocessing_model_name}\",\n",
    "     \"fair-rank\": True,\n",
    "    \n",
    "    \"log_name\": f\"{postprocessing_model_name}_{dataset_name}\", # path to save the evaluation and the output\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/postprocessing_with_fairdiverse.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_model, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Step 2: Run the post-processing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your training config...\n",
      "{'ranking_store_path': '2025-5-30_SASRec_ml-100k', 'model': 'CPFair', 'fair-rank': True, 'log_name': 'CPFair_ml-100k', 'topk': [5, 10, 20], 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'ml-100k', 'stage': 'post-processing', 'task': 'recommendation'}\n",
      "your args: Namespace(task='recommendation', stage='post-processing', dataset='ml-100k', train_config_file='postprocessing_with_fairdiverse.yaml')\n",
      "start to load config...\n",
      "start to load model...\n",
      "your loading config is:\n",
      "{'batch_size': 64, 'data_type': 'sequential', 'dataset': 'ml-100k', 'decimals': 4, 'device': 'cpu', 'epoch': 20, 'eval_step': 5, 'eval_type': 'ranking', 'fair-rank': True, 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'group_aggregation_threshold': 15, 'group_id': 'first_class:token', 'group_num': 14, 'group_val': 5, 'history_field': 'history_behaviors', 'history_length': 20, 'item_domain': 'movie', 'item_id': 'item_id:token', 'item_num': 1345, 'item_val': 5, 'label_id': 'label:float', 'label_threshold': 3, 'learning_rate': 0.001, 'log_name': 'CPFair_ml-100k', 'mmf_eval_ratio': 0.2, 'model': 'CPFair', 'reprocess': True, 'sample_num': 350, 'sample_size': 1.0, 'stage': 'post-processing', 'store_scores': True, 'task': 'recommendation', 'test_ratio': 0.1, 'text_id': 'movie_title:token_seq', 'timestamp': 'timestamp:float', 'topk': [5, 10, 20], 'use_llm': False, 'user_id': 'user_id:token', 'user_num': 822, 'user_val': 5, 'valid_ratio': 0.1, 'watch_metric': 'ndcg@5', 'lambda': 1, 'eval_batch_size': 128, 'ranking_store_path': '2025-5-30_SASRec_ml-100k'}\n",
      "loading ranking scores....\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 16323.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 17899.05it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 18429.52it/s]\n",
      "{'ndcg@5': np.float64(0.9964), 'u_loss@5': np.float64(0.0092), 'MinMaxRatio@5': np.float64(0.3293), 'MMF@5': np.float64(0.3614), 'GINI@5': np.float64(0.1928), 'Entropy@5': np.float64(3.7234), 'ndcg@10': np.float64(0.996), 'u_loss@10': np.float64(0.0111), 'MinMaxRatio@10': np.float64(0.2246), 'MMF@10': np.float64(0.3007), 'GINI@10': np.float64(0.2684), 'Entropy@10': np.float64(3.6429), 'ndcg@20': np.float64(0.9953), 'u_loss@20': np.float64(0.0135), 'MinMaxRatio@20': np.float64(0.1303), 'MMF@20': np.float64(0.2407), 'GINI@20': np.float64(0.3675), 'Entropy@20': np.float64(3.4809)}\n",
      "result and config dump in recommendation/log/2025-5-30_CPFair_ml-100k\n"
     ]
    }
   ],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"{dataset_name}\" --train_config_file \"postprocessing_with_fairdiverse.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Evaluation Resultsüìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "### NDCG as a Measure of Utility Loss\n",
    "\n",
    "Here, **Normalized Discounted Cumulative Gain (NDCG)** is used to quantify the **loss in utility** resulting from the post-processing intervention.\n",
    "\n",
    "Specifically, it compares the ranking produced by **CP-Fair** with the original ranking of the **base model** (e.g., *SASRec*).\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{Mean\\_NDCG@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{DCG_u}{IDCG_u}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  *U* is the set of users,\n",
    "- **DCG** is computed based on the ranking produced by the post-processing intervention (e.g. CP-Fair),\n",
    "- **Ideal DCG** is computed based on the original ranking produced by the base model (e.g. SASRec).\n",
    "\n",
    "An NDCG closer to 1 indicates minimal loss in utility due to the intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "### Mean Utility Loss\n",
    "\n",
    "The **mean utility loss at rank k** across all users is defined as:\n",
    "\n",
    "$$\n",
    "U_{loss@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\left[ \\frac{1}{k} \\left( \\sum_{i=1}^{k} \\text{score}_{base} {(u,i)} - \\sum_{i=1}^{k} \\text{score}_{post} {(u,i)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- *U* is the set of users,\n",
    "- $ \\text{score}_{base} {(u,i)} $  is the score assigned to the *i-th* item in the **base model's** top-*k* ranking for user *u*,\n",
    "- $ \\text{score}_{post} {(u,i)} $ is the score of the *i-th* item in the **post-processing model's** top-*k* ranking for user *u*.\n",
    "\n",
    "This metric captures the **average per-item utility loss over all users**, reflecting how much the re-ranking procedure deviates from the base model in terms of utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         0.9964  0.9960  0.9953\n",
      "u_loss       0.0092  0.0111  0.0135\n",
      "MinMaxRatio  0.3293  0.2246  0.1303\n",
      "MMF          0.3614  0.3007  0.2407\n",
      "GINI         0.1928  0.2684  0.3675\n",
      "Entropy      3.7234  3.6429  3.4809\n"
     ]
    }
   ],
   "source": [
    "# evaluation results of post-processing model\n",
    "print_evaluation_results(postprocessing_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         0.3337  0.3520  0.3584\n",
      "mrr          0.4371  0.4495  0.4538\n",
      "hr           0.3390  0.3823  0.4100\n",
      "mmf          0.0648  0.0724  0.0890\n",
      "gini         0.6563  0.6403  0.6193\n",
      "entropy      2.6317  2.6748  2.7412\n",
      "maxminratio  0.0046  0.0050  0.0223\n"
     ]
    }
   ],
   "source": [
    "# evaluation results of the base model\n",
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ CP-Fair improves fairness and diversity metrics over the base model SASRec, with only a small drop in NDCG and utility loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **3.2 Withoout Input from FairDiverse**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "To simulate a scenario where you did not use FairDiverse to generate the required files let's rename the already generated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(\"recommendation/processed_dataset/ml-100k\", \"recommendation/processed_dataset/ml-100k_fairdiverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Expected Data Format**\n",
    "\n",
    "If you want to use a model not supported by FairDiverse and run the evaluation metrics you need to have the following files:\n",
    "\n",
    "(1) `iid2pid.json` - Mapping from item ID to provider/group ID \n",
    "\n",
    "(2) `ranking_scores.npz` - Numpy array of ranking scores\n",
    "\n",
    "Otherwise use one of the Base Models or In-processing models supported by FairDiverse to generate those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Example of expected format for iid2pid.json file -- item_id:group_id\n",
    "iid2pid = {\"1488\": \"0\", \"42\": \"1\", \"2508\": \"2\", \"1084\": \"3\", \"1182\": \"0\", \"1468\": \"4\", \"2087\": \"3\", \"153\": \"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40474748 0.2633356  0.1031167  ... 0.29658107 0.09156237 0.54570303]\n",
      " [0.42015236 0.19011406 0.37611625 ... 0.50458316 0.44335377 0.28466151]\n",
      " [0.60806535 0.6007739  0.53476787 ... 0.86598956 0.36985042 0.76051944]\n",
      " ...\n",
      " [0.49233631 0.86766064 0.28818561 ... 0.63489722 0.2895405  0.17105741]\n",
      " [0.95179313 0.1995245  0.64154733 ... 0.36000267 0.2854486  0.24324722]\n",
      " [0.93102505 0.55022894 0.10935651 ... 0.05804184 0.36526744 0.48128691]]\n",
      "Shape: (50, 100)\n"
     ]
    }
   ],
   "source": [
    "# (2) Example of expected format for ranking_scores.npz file -- sparse matrix of users x items and the corresponding score\n",
    "\n",
    "n_users = 50\n",
    "n_items = 100\n",
    "user_item_matrix = np.random.rand(n_users, n_items)\n",
    "print(user_item_matrix)\n",
    "print(\"Shape:\", user_item_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Create the files needed for running the post-processing method.\n",
    "\n",
    "To simulate a scenarion where you did not use FairDiverse to create the files, take the files generated by the previous base model (e.g. SASRec) and follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 2:** Place `ranking_scores.npz` under `~/recommendation/log/ml-100k`\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄlog/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ ranking_scores.npz     # Numpy array of ranking scores\n",
    "\n",
    "```\n",
    "#### **Step 3:** Place `iid2pid.json` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ iid2pid.json    # Mapping from item ID to provider/group ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 4:** Save data configuration file `process_config.yaml` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ process_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"recommendation/log/{dataset_name}\", exist_ok=True)\n",
    "os.makedirs(f\"recommendation/processed_dataset/{dataset_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'user_id:token', 'item_id': 'item_id:token', 'group_id': 'first_class:token', 'label_id': 'rating:float', 'timestamp': 'timestamp:float', 'text_id': 'movie_title:token_seq', 'label_threshold': 3, 'item_domain': 'movie', 'item_val': 5, 'user_val': 5, 'group_val': 5, 'group_aggregation_threshold': 15, 'sample_size': 1.0, 'valid_ratio': 0.1, 'test_ratio': 0.1, 'reprocess': True, 'sample_num': 350, 'history_length': 20, 'item_num': 1345, 'user_num': 822, 'group_num': 14}\n"
     ]
    }
   ],
   "source": [
    "# run this if you did not use FairDiverse to create the score file\n",
    "num_users = 822 # n rows of the matrix \n",
    "num_items = 1345 # n columns of the matrix\n",
    "num_groups = 14 # this should correspond to the unique values from iid2pid.json \n",
    "config_data[\"item_num\"] = num_items\n",
    "config_data[\"user_num\"] = num_users\n",
    "config_data[\"group_num\"] = num_groups\n",
    "\n",
    "print(config_data)\n",
    "with open(f\"recommendation/processed_dataset/{dataset_name}/process_config.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_data, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 5:** Create a configuration file for running a post-processing intervention under \n",
    "You can change parameters specific to each model in the following configuration file: `recommendation/properties/models/<model_name>.yaml` \n",
    "```yaml\n",
    "{\n",
    "   ###############the ranking score stored path for the post-processing##################\n",
    "   ranking_store_path: \"ml-100k\", \n",
    "   #######################################################################################\n",
    "\n",
    "   ### !!! Don't change - needs to be set to False as we don't run a post-processing intervention !!!\n",
    "   model: \"CPFair\",\n",
    "   log_name: \"CPFair_ml-100k\",\n",
    "\n",
    "   #########################Evaluation parameters#########################################\n",
    "   topk: [5, 10, 20],\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   #####################################################################################\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_model_name = \"CPFair\"\n",
    "\n",
    "config_model = {\n",
    "    \"ranking_store_path\": f\"{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Change to any of the supported post-processing methods in Fairdiverse\n",
    "    \"model\": f\"{postprocessing_model_name}\",\n",
    "    \"fair-rank\": True,\n",
    "    \n",
    "    \"log_name\": f\"{postprocessing_model_name}_without_fairdiverse_{dataset_name}\", # path to save the evaluation and the output\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/postprocessing_without_fairdiverse.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_model, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 6:** Run the post-processing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your training config...\n",
      "{'ranking_store_path': 'ml-100k', 'model': 'CPFair', 'fair-rank': True, 'log_name': 'CPFair_without_fairdiverse_ml-100k', 'topk': [5, 10, 20], 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'ml-100k', 'stage': 'post-processing', 'task': 'recommendation'}\n",
      "your args: Namespace(task='recommendation', stage='post-processing', dataset='ml-100k', train_config_file='postprocessing_without_fairdiverse.yaml')\n",
      "start to load config...\n",
      "start to load model...\n",
      "your loading config is:\n",
      "{'user_id': 'user_id:token', 'item_id': 'item_id:token', 'group_id': 'first_class:token', 'label_id': 'rating:float', 'timestamp': 'timestamp:float', 'text_id': 'movie_title:token_seq', 'label_threshold': 3, 'item_domain': 'movie', 'item_val': 5, 'user_val': 5, 'group_val': 5, 'group_aggregation_threshold': 15, 'sample_size': 1.0, 'valid_ratio': 0.1, 'test_ratio': 0.1, 'reprocess': True, 'sample_num': 350, 'history_length': 20, 'item_num': 1345, 'user_num': 822, 'group_num': 14, 'learning_rate': 0.001, 'batch_size': 64, 'lambda': 1, 'eval_step': 5, 'eval_type': 'ranking', 'eval_batch_size': 128, 'watch_metric': 'ndcg@5', 'topk': [5, 10, 20], 'store_scores': True, 'decimals': 4, 'mmf_eval_ratio': 0.2, 'fairness_type': 'Exposure', 'device': 'cpu', 'epoch': 20, 'ranking_store_path': 'ml-100k', 'model': 'CPFair', 'fair-rank': True, 'log_name': 'CPFair_without_fairdiverse_ml-100k', 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'dataset': 'ml-100k', 'stage': 'post-processing', 'task': 'recommendation'}\n",
      "loading ranking scores....\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 16049.77it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 16710.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 17423.20it/s]\n",
      "{'ndcg@5': np.float64(0.9964), 'u_loss@5': np.float64(0.0092), 'MinMaxRatio@5': np.float64(0.3293), 'MMF@5': np.float64(0.3614), 'GINI@5': np.float64(0.1928), 'Entropy@5': np.float64(3.7234), 'ndcg@10': np.float64(0.996), 'u_loss@10': np.float64(0.0111), 'MinMaxRatio@10': np.float64(0.2246), 'MMF@10': np.float64(0.3007), 'GINI@10': np.float64(0.2684), 'Entropy@10': np.float64(3.6429), 'ndcg@20': np.float64(0.9953), 'u_loss@20': np.float64(0.0135), 'MinMaxRatio@20': np.float64(0.1303), 'MMF@20': np.float64(0.2407), 'GINI@20': np.float64(0.3675), 'Entropy@20': np.float64(3.4809)}\n",
      "result and config dump in recommendation/log/2025-5-30_CPFair_without_fairdiverse_ml-100k\n"
     ]
    }
   ],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"{dataset_name}\" --train_config_file \"postprocessing_without_fairdiverse.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "##### **Evaluation Results üìà** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG as a Measure of Utility Loss\n",
    "\n",
    "Here, **Normalized Discounted Cumulative Gain (NDCG)** is used to quantify the **loss in utility** resulting from the post-processing intervention.\n",
    "\n",
    "Specifically, it compares the ranking produced by **CP-Fair** with the original ranking of the **base model** (e.g., *SASRec*).\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{Mean\\_NDCG@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{DCG_u}{IDCG_u}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  *U* is the set of users,\n",
    "- **DCG** is computed based on the ranking produced by the post-processing intervention (e.g. CP-Fair),\n",
    "- **Ideal DCG** is computed based on the original ranking produced by the base model (e.g. SASRec).\n",
    "\n",
    "An NDCG closer to 1 indicates minimal loss in utility due to the intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Utility Loss\n",
    "\n",
    "The **mean utility loss at rank k** across all users is defined as:\n",
    "\n",
    "$$\n",
    "U_{loss@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\left[ \\frac{1}{k} \\left( \\sum_{i=1}^{k} \\text{score}_{base} {(u,i)} - \\sum_{i=1}^{k} \\text{score}_{post} {(u,i)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- *U* is the set of users,\n",
    "- $ \\text{score}_{base} {(u,i)} $  is the score assigned to the *i-th* item in the **base model's** top-*k* ranking for user *u*,\n",
    "- $ \\text{score}_{post} {(u,i)} $ is the score of the *i-th* item in the **post-processing model's** top-*k* ranking for user *u*.\n",
    "\n",
    "This metric captures the **average per-item utility loss over all users**, reflecting how much the re-ranking procedure deviates from the base model in terms of utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         0.9964  0.9960  0.9953\n",
      "u_loss       0.0092  0.0111  0.0135\n",
      "MinMaxRatio  0.3293  0.2246  0.1303\n",
      "MMF          0.3614  0.3007  0.2407\n",
      "GINI         0.1928  0.2684  0.3675\n",
      "Entropy      3.7234  3.6429  3.4809\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(postprocessing_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         0.3337  0.3520  0.3584\n",
      "mrr          0.4371  0.4495  0.4538\n",
      "hr           0.3390  0.3823  0.4100\n",
      "mmf          0.0648  0.0724  0.0890\n",
      "gini         0.6563  0.6403  0.6193\n",
      "entropy      2.6317  2.6748  2.7412\n",
      "maxminratio  0.0046  0.0050  0.0223\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **4. Run Evaluation üìà**\n",
    "\n",
    "---\n",
    "\n",
    "If you want to use a model not supported by FairDiverse and run the evaluation metrics you need to have the following files:\n",
    "\n",
    "(1) `iid2pid.json` - Mapping from item ID to provider/group ID \n",
    "\n",
    "(2) `ranking_scores.npz` - Numpy array of ranking scores\n",
    "\n",
    "To simulate a scenario where you did not use FairDiverse to generate the required files let's rename the already generated folder for the base model, and run again only the evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run if you did this in Section 3.1\n",
    "# os.rename(\"recommendation/processed_dataset/ml-100k\", \"recommendation/processed_dataset/ml-100k_fairdiverse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Example of expected format for iid2pid.json file -- item_id:group_id\n",
    "iid2pid = {\"1488\": \"0\", \"42\": \"1\", \"2508\": \"2\", \"1084\": \"3\", \"1182\": \"0\", \"1468\": \"4\", \"2087\": \"3\", \"153\": \"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61946556 0.19534751 0.88533229 ... 0.95538344 0.2630665  0.40493939]\n",
      " [0.89658711 0.07636513 0.74548944 ... 0.8774491  0.81911061 0.33370351]\n",
      " [0.3310386  0.86640421 0.87686277 ... 0.05067934 0.43311222 0.66678636]\n",
      " ...\n",
      " [0.11101553 0.10151666 0.97995196 ... 0.68501064 0.12409233 0.51993124]\n",
      " [0.74174185 0.46589591 0.61917081 ... 0.12416675 0.45410437 0.94596302]\n",
      " [0.89267678 0.15557418 0.68177919 ... 0.77270585 0.65020475 0.727559  ]]\n",
      "Shape: (50, 100)\n"
     ]
    }
   ],
   "source": [
    "# (2) Example of expected format for ranking_scores.npz file -- sparse matrix of users x items and the corresponding score\n",
    "\n",
    "users = 50\n",
    "items = 100\n",
    "user_item_matrix = np.random.rand(users, items)\n",
    "print(user_item_matrix)\n",
    "print(\"Shape:\", user_item_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Create the files needed for running the evaluation\n",
    "\n",
    "To simulate a scenarion where you did not use FairDiverse to create the files, take the files generated by the previous base model (e.g. SASRec) and follow the steps below.\n",
    "#### **Step 2:** Place `ranking_scores.npz` under `~/recommendation/log/SASRec_ml-100k`\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄlog/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ ranking_scores.npz     # Numpy array of ranking scores\n",
    "\n",
    "```\n",
    "#### **Step 3:** Place `iid2pid.json` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ iid2pid.json    # Mapping from item ID to provider/group ID\n",
    "```\n",
    "\n",
    "\n",
    "#### **Step 4:** Save data configuration file `process_config.yaml` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ process_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"recommendation/log/{base_model_name}_{dataset_name}\", exist_ok=True)\n",
    "os.makedirs(f\"recommendation/processed_dataset/{dataset_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'user_id:token', 'item_id': 'item_id:token', 'group_id': 'first_class:token', 'label_id': 'rating:float', 'timestamp': 'timestamp:float', 'text_id': 'movie_title:token_seq', 'label_threshold': 3, 'item_domain': 'movie', 'item_val': 5, 'user_val': 5, 'group_val': 5, 'group_aggregation_threshold': 15, 'sample_size': 1.0, 'valid_ratio': 0.1, 'test_ratio': 0.1, 'reprocess': True, 'sample_num': 350, 'history_length': 20, 'item_num': 1345, 'user_num': 822, 'group_num': 14}\n"
     ]
    }
   ],
   "source": [
    "# run this if you did not use FairDiverse to create the score file\n",
    "num_users = 822 # n rows of the matrix \n",
    "num_items = 1345 # n columns of the matrix\n",
    "num_groups = 14 # this should correspond to the unique values from iid2pid.json \n",
    "config_data[\"item_num\"] = num_items\n",
    "config_data[\"user_num\"] = num_users\n",
    "config_data[\"group_num\"] = num_groups\n",
    "\n",
    "print(config_data)\n",
    "with open(f\"recommendation/processed_dataset/{dataset_name}/process_config.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_data, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 5:** Create a configuration file for the evaluation\n",
    "```yaml\n",
    "{\n",
    "   ###############the ranking score stored path for the post-processing##################\n",
    "   ranking_store_path: \"SASRec_ml-100k\", \n",
    "   #######################################################################################\n",
    "\n",
    "   ### !!! Don't change - needs to be set to False as we don't run a post-processing intervention !!!\n",
    "   model: False,\n",
    "   use_llm: False,\n",
    "   ###############eval output path##################\n",
    "   log_name: \"eval_ml-100k\",\n",
    "\n",
    "   #########################Evaluation parameters#########################################\n",
    "   topk: [5, 10, 20],\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   #####################################################################################\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_eval = {\n",
    "    \"ranking_store_path\": f\"{base_model_name}_{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Do not change ‚Äî no post-processing model used, and no base model used as we want to just perform evaluation\n",
    "    \"fair-rank\": False,\n",
    "    # output file for evaluation results\n",
    "    \"log_name\": f\"eval_{base_model_name}_without_fairdiverse_{dataset_name}\", # path to save the evaluation\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"./recommendation/evaluation.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_eval, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 6:** Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your training config...\n",
      "{'ranking_store_path': 'SASRec_ml-100k', 'fair-rank': False, 'log_name': 'eval_SASRec_without_fairdiverse_ml-100k', 'topk': [5, 10, 20], 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'ml-100k', 'stage': 'post-processing', 'task': 'recommendation'}\n",
      "your args: Namespace(task='recommendation', stage='post-processing', dataset='ml-100k', train_config_file='evaluation.yaml')\n",
      "start to load config...\n",
      "your loading config is:\n",
      "{'user_id': 'user_id:token', 'item_id': 'item_id:token', 'group_id': 'first_class:token', 'label_id': 'rating:float', 'timestamp': 'timestamp:float', 'text_id': 'movie_title:token_seq', 'label_threshold': 3, 'item_domain': 'movie', 'item_val': 5, 'user_val': 5, 'group_val': 5, 'group_aggregation_threshold': 15, 'sample_size': 1.0, 'valid_ratio': 0.1, 'test_ratio': 0.1, 'reprocess': True, 'sample_num': 350, 'history_length': 20, 'item_num': 1345, 'user_num': 822, 'group_num': 14, 'eval_step': 5, 'eval_type': 'ranking', 'eval_batch_size': 128, 'watch_metric': 'ndcg@5', 'topk': [5, 10, 20], 'store_scores': True, 'decimals': 4, 'mmf_eval_ratio': 0.2, 'fairness_type': 'Exposure', 'device': 'cpu', 'batch_size': 64, 'learning_rate': 0.001, 'epoch': 20, 'ranking_store_path': 'SASRec_ml-100k', 'fair-rank': False, 'log_name': 'eval_SASRec_without_fairdiverse_ml-100k', 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'dataset': 'ml-100k', 'stage': 'post-processing', 'task': 'recommendation'}\n",
      "loading ranking scores....\n",
      "No model loaded!\n",
      "{'ndcg@5': np.float64(1.0), 'u_loss@5': np.float64(0.0), 'MinMaxRatio@5': np.float64(0.0046), 'MMF@5': np.float64(0.0648), 'GINI@5': np.float64(0.6563), 'Entropy@5': np.float64(2.6317), 'ndcg@10': np.float64(1.0), 'u_loss@10': np.float64(0.0), 'MinMaxRatio@10': np.float64(0.005), 'MMF@10': np.float64(0.0724), 'GINI@10': np.float64(0.6403), 'Entropy@10': np.float64(2.6748), 'ndcg@20': np.float64(1.0), 'u_loss@20': np.float64(0.0), 'MinMaxRatio@20': np.float64(0.0223), 'MMF@20': np.float64(0.089), 'GINI@20': np.float64(0.6193), 'Entropy@20': np.float64(2.7412)}\n",
      "result and config dump in recommendation/log/2025-5-30_eval_SASRec_without_fairdiverse_ml-100k\n"
     ]
    }
   ],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"{dataset_name}\" --train_config_file \"evaluation.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "##### **Evaluation Resultsüìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         1.0000  1.0000  1.0000\n",
      "u_loss       0.0000  0.0000  0.0000\n",
      "MinMaxRatio  0.0046  0.0050  0.0223\n",
      "MMF          0.0648  0.0724  0.0890\n",
      "GINI         0.6563  0.6403  0.6193\n",
      "Entropy      2.6317  2.6748  2.7412\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(f\"eval_{base_model_name}_without_fairdiverse\", dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **5. Add Post-processing Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "RAIF is a reranking approach based on MILP that selects N items from K candidates for each user. Its fairness objective aims to balance the overall exposure among different item groups. Now we introduce a new optimization objective‚Äîminimizing the disparity in average exposure across groups‚Äîand refer to this enhanced method as RAIFPro. These are the steps for adding RAIFPro to FairDiverse. You could also add your own post-processing model in the similar way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Create a python file inside: `fairdiverse/recommendation/rerank_model` which should have the name of the model (e.g. RAIFPro.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(os.path.join(\"recommendation/rerank_model\", \"RAIFPro.py\"), 'a').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Step 2:** Implement a class which inherits `Abstract_Reranker` with the name of the model (e.g. RAIFPro). You can use the common parameters within the `Abstract_Reranker` class.\n",
    "\n",
    "Input:\n",
    "\n",
    "    relevance: numpy.ndarray, shape (num_users, num_items)\n",
    "        A 2D array where each row corresponds to a user and contains item relevance scores.\n",
    "    topk: int\n",
    "        The number of top-ranked items to select per user.\n",
    "\n",
    "\n",
    "Output: \n",
    "\n",
    "    rerank_list: list of list of int, shape (num_users, size)\n",
    "        A list where each entry contains exactly `size` selected items for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting recommendation/rerank_model/RAIFPro.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile recommendation/rerank_model/RAIFPro.py\n",
    "\n",
    "import numpy as np\n",
    "from .Abstract_Reranker import Abstract_Reranker\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "r\"\"\"\n",
    "RAIFPro changes the RAIF's optimization objective to minimizing the disparity in average exposure across groups.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_results(num_users, size, topk, solution, topk_items):\n",
    "    \"\"\"\n",
    "    Converts the solution matrix into selected item lists for multiple users.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_users: int\n",
    "        The number of users.\n",
    "    size: int\n",
    "        The expected number of items per user in the final rerank list.\n",
    "    topk: int\n",
    "        The number of candidate items per user.\n",
    "    solution: numpy.ndarray, shape (num_users, topk)\n",
    "        A matrix indicating the final selected items.\n",
    "    topk_items: list of list of int, shape (num_users, topk)\n",
    "        A list where each entry contains candidate item IDs corresponding to a user.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    rerank: list of list of int, shape (num_users, size)\n",
    "        A list where each entry contains exactly `size` selected items for a user.\n",
    "    \"\"\"\n",
    "  \n",
    "    rerank = []\n",
    "    for i in range(num_users):\n",
    "\n",
    "        rerank_user = []\n",
    "        for j in range(topk):\n",
    "            if solution[i, j] > 0.5:\n",
    "                rerank_user.append(topk_items[i][j])\n",
    "\n",
    "        assert len(rerank_user) == size\n",
    "        rerank.append([int(x) for x in rerank_user])\n",
    "\n",
    "    return rerank\n",
    "\n",
    "def load_ranking_matrices(relevance, topk): \n",
    "    \"\"\"\n",
    "    Generates ranking matrices by selecting the top-k relevant items for each user.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    relevance: numpy.ndarray, shape (num_users, num_items)\n",
    "        A 2D array where each row corresponds to a user and contains item relevance scores.\n",
    "    topk: int\n",
    "        The number of top-ranked items to select per user.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    topk_items: numpy.ndarray, shape (num_users, topk)\n",
    "        A 2D array where each row contains the indices of the top-k items for the corresponding user.\n",
    "    topk_scores: numpy.ndarray, shape (num_users, topk)\n",
    "        A 2D array where each row contains the relevance scores of the selected top-k items.\n",
    "    num_users: int\n",
    "        The total number of users.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    num_users, num_items = relevance.shape\n",
    "    \n",
    "    topk_items = np.zeros((num_users, topk), dtype=int)\n",
    "    topk_scores = np.zeros((num_users, topk))\n",
    "\n",
    "    for user_idx in range(num_users):\n",
    "        # Get the indices of the items sorted by their relevance score in descending order\n",
    "        sorted_indices = np.argsort(relevance[user_idx])[::-1]\n",
    "        \n",
    "        # Select the top k indices and corresponding scores\n",
    "        topk_items[user_idx] = sorted_indices[:topk]\n",
    "        topk_scores[user_idx] = relevance[user_idx, sorted_indices[:topk]]\n",
    "    \n",
    "    return topk_items, topk_scores, num_users\n",
    "\n",
    "def read_item_index(total_users, topk, no_item_groups, item_group_map, topk_items):\n",
    "    \"\"\"\n",
    "    Creates a binary indicator matrix that maps items to their respective item groups.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    total_users: int\n",
    "        The total number of users.\n",
    "    topk: int\n",
    "        The number of candidate items per user.\n",
    "    no_item_groups: int\n",
    "        The total number of item groups.\n",
    "    item_group_map: dict\n",
    "        A dictionary mapping item indices to their corresponding group IDs.\n",
    "    topk_items: list of list of int, shape (total_users, topk)\n",
    "        A list where each entry contains candidate item IDs corresponding to a user.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Ihelp: numpy.ndarray, shape (total_users, topk, no_item_groups)\n",
    "        A binary 3D array where `Ihelp[uid][lid][k] = 1` if the `lid`-th item for user `uid`\n",
    "        belongs to item group `k`, otherwise `0`.\n",
    "    \"\"\"\n",
    "\n",
    "    Ihelp = np.zeros((total_users, topk, no_item_groups))\n",
    "    for uid in range(total_users):\n",
    "        for lid in range(topk):\n",
    "            for k in range(no_item_groups):\n",
    "                top_ = topk_items[uid][lid]\n",
    "                if top_ in item_group_map.keys():\n",
    "                    if item_group_map[topk_items[uid][lid]] == k:\n",
    "                        Ihelp[uid][lid][k] = 1\n",
    "\n",
    "    return Ihelp\n",
    "\n",
    "def fairness_optimisation(total_users, alpha, size, topk, group_num, Ihelp, topk_scores, mean):\n",
    "    \"\"\"\n",
    "    Solves a fairness-aware ranking optimization problem using Gurobi.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    total_users: int\n",
    "        The total number of users.\n",
    "    alpha: float\n",
    "        The fairness regularization parameter. A higher alpha increases fairness consideration.\n",
    "    size: int\n",
    "        The number of items to be selected per user.\n",
    "    topk: int\n",
    "        The number of candidate items per user.\n",
    "    group_num: int\n",
    "        The number of item groups.\n",
    "    Ihelp: numpy.ndarray, shape (total_users, topk, group_num)\n",
    "        A binary indicator matrix.\n",
    "    topk_scores: numpy.ndarray, shape (total_users, topk)\n",
    "        A 2D relevance score matrix.\n",
    "    mean: list\n",
    "        The ideal exposure across item groups.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    solution: numpy.ndarray, shape (num_users, topk)\n",
    "        A matrix indicating the final selected items.\n",
    "  \n",
    "    \"\"\"\n",
    "  \n",
    "    print(f\"Running RAIFPro, {format(alpha, 'f')}\")\n",
    "    # V1: No. of users\n",
    "    # V2: No. of top items (topk)\n",
    "    # V4: no. of item groups\n",
    "    V1, V2, V4 = range(total_users), range(topk), range(group_num)\n",
    "\n",
    "    # initiate model\n",
    "    model = Model()\n",
    "\n",
    "    W = model.addVars(V1, V2, vtype=GRB.BINARY)\n",
    "    item_group = model.addVars(V4, vtype=GRB.CONTINUOUS)\n",
    "    item_fair = model.addVar(vtype=GRB.CONTINUOUS)\n",
    "    abs_diff = model.addVars(V4, lb=0, name=\"abs_diff\")\n",
    "               \n",
    "    model.setObjective(quicksum(topk_scores[i][j] * W[i, j] for i in V1 for j in V2) - alpha * item_fair, GRB.MAXIMIZE)\n",
    "\n",
    "    for i in V1:\n",
    "        model.addConstr(quicksum(W[i, j] for j in V2) == size)\n",
    "    \n",
    "    for k in V4:\n",
    "        model.addConstr(item_group[k] == quicksum(W[i, j] * Ihelp[i][j][k] for i in V1 for j in V2))\n",
    "    \n",
    "    for k in V4:\n",
    "        model.addConstr(abs_diff[k] >= item_group[k] - mean[k])\n",
    "        model.addConstr(abs_diff[k] >= -(item_group[k] - mean[k]))\n",
    "\n",
    "    model.addConstr(item_fair == quicksum(abs_diff[k] for k in V4))\n",
    "\n",
    "\n",
    "    # optimizing\n",
    "    model.optimize()\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        solution = model.getAttr('x', W)\n",
    "        #fairness = model.getAttr('x', item_group)\n",
    "\n",
    "\n",
    "    return solution\n",
    "\n",
    "def ideal(matrix, num_users, k):\n",
    "\n",
    "    group_count = [sum(row[i] for row in matrix) for i in range(len(matrix[0]))]\n",
    "    total = sum(group_count)\n",
    "    exposure = num_users * k\n",
    "    distribution = [x / total * exposure for x in group_count]\n",
    "\n",
    "    return distribution \n",
    "\n",
    "class RAIFPro(Abstract_Reranker):\n",
    "    def __init__(self, config, weights = None):\n",
    "        super().__init__(config, weights)\n",
    "\n",
    "\n",
    "    def rerank(self, ranking_score, k):\n",
    "        ## its parameters\n",
    "        topk = self.config['candidate']\n",
    "        alpha = self.config['alpha']\n",
    "\n",
    "        topk_items, topk_scores, num_users = load_ranking_matrices(ranking_score, topk)\n",
    "\n",
    "        #ideal exposure across groups\n",
    "        mean = ideal(self.M, num_users, k)\n",
    "\n",
    "        Ihelp = read_item_index(total_users=num_users, topk=topk, no_item_groups=self.group_num, item_group_map=self.iid2pid, topk_items=topk_items) \n",
    "        solution = fairness_optimisation(num_users, alpha, k, topk, self.group_num, Ihelp, topk_scores, mean)\n",
    "        rerank_list = get_results(num_users, k, topk, solution, topk_items)\n",
    "        \n",
    "        return rerank_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Step 3:** Create the configuration file under `FairDiverse-master/fairdiverse/recommendation/properties/models/` with the name of the model.yaml (e.g. RAIFPro.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "congif_raifpro = {\n",
    "    \"alpha\": 0.2,      #the weight parameter of item fairness term\n",
    "    \"candidate\": 100   #the number of item candidates \n",
    "}\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"recommendation/properties/models/RAIFPro.yaml\"\n",
    "\n",
    "\n",
    "# Write the dictionary to the YAML file\n",
    "with open(file_path, 'w') as file:\n",
    "    yaml.dump(congif_raifpro, file, default_flow_style=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Create a running configuration file `postprocessing_new_model.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RAIFPro\"\n",
    "\n",
    "today = date.today()\n",
    "today_format = f\"{today.year}-{today.month}-{today.day}\"\n",
    " \n",
    "config_new_model = {\n",
    "    \"ranking_store_path\": f\"{today_format}_{base_model_name}_{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Change to any of the supported post-processing methods in Fairdiverse\n",
    "    \"model\": f\"{model_name}\",\n",
    "    \"fair-rank\": True,\n",
    "    \"log_name\": f\"{model_name}_{dataset_name}\", # path to save the evaluation\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "with open(f\"recommendation/postprocessing_new_model.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_new_model, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Import your custom model package in the corresponding file `fairdiverse/recommendation/rerank_model/__init__.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"recommendation/rerank_model/__init__.py\"\n",
    "\n",
    "# Read the current contents\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Define the new line to append\n",
    "new_import = \"\\nfrom .RAIFPro import RAIFPro\\n\"\n",
    "\n",
    "# Append only if it's not already present\n",
    "if new_import not in lines:\n",
    "    lines.append(new_import)\n",
    "\n",
    "# Write back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Define the model in the script `FairDiverse-master/fairdiverse/recommendation/reranker.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAIFPro support added successfully.\n"
     ]
    }
   ],
   "source": [
    "# Path to the target Python file\n",
    "file_path = \"recommendation/reranker.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Step 1: Update import line\n",
    "for i, line in enumerate(lines):\n",
    "    if \"from .rerank_model import\" in line:\n",
    "        if \"RAIFPro\" not in line:\n",
    "            lines[i] = line.strip() + \", RAIFPro\\n\"\n",
    "        break  # Only modify the first matching import line\n",
    "\n",
    "# Step 2: Add the RAIFPro elif clause\n",
    "new_elif_block = [\n",
    "    \"elif config['model'] == 'RAIFPro':\\n\",\n",
    "    \"    Reranker = RAIFPro(config)\\n\"\n",
    "]\n",
    "\n",
    "# Insert just before the existing \"else:\" clause inside the rerank method\n",
    "for i, line in enumerate(lines):\n",
    "    if \"else:\" in line and \"raise NotImplementedError\" in lines[i + 1]:\n",
    "        indent = \" \" * (len(line) - len(line.lstrip()))\n",
    "        # Make sure to adjust indentation to match\n",
    "        lines[i:i] = [indent + l for l in new_elif_block]\n",
    "        break\n",
    "\n",
    "# Save the modified file\n",
    "with open(file_path, 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "print(\"RAIFPro support added successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7:** To solve RAIFPro, we need install and set Gurobi.\n",
    "\n",
    "1. install packages in environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mip in /Users/clararus/anaconda3/envs/fair_diverse/lib/python3.9/site-packages (1.15.0)\n",
      "Requirement already satisfied: cffi==1.15.* in /Users/clararus/anaconda3/envs/fair_diverse/lib/python3.9/site-packages (from mip) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/clararus/anaconda3/envs/fair_diverse/lib/python3.9/site-packages (from cffi==1.15.*->mip) (2.22)\n",
      "Requirement already satisfied: gurobipy in /Users/clararus/anaconda3/envs/fair_diverse/lib/python3.9/site-packages (12.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mip\n",
    "!pip install gurobipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. install Gurobi license \"Named-User Academic\" to your laptop\n",
    "https://portal.gurobi.com/iam/licenses/request\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grbgetkey xxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8:** Run RAIFPro for fairness-aware reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your training config...\n",
      "{'ranking_store_path': '2025-5-30_SASRec_ml-100k', 'model': 'RAIFPro', 'fair-rank': True, 'log_name': 'RAIFPro_ml-100k', 'topk': [5, 10, 20], 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'steam', 'stage': 'post-processing', 'task': 'recommendation'}\n",
      "your args: Namespace(task='recommendation', stage='post-processing', dataset='steam', train_config_file='postprocessing_new_model.yaml')\n",
      "start to load config...\n",
      "start to load model...\n",
      "your loading config is:\n",
      "{'batch_size': 64, 'data_type': 'pair', 'dataset': 'steam', 'decimals': 4, 'device': 'cpu', 'epoch': 20, 'eval_step': 5, 'eval_type': 'ranking', 'fair-rank': True, 'fair_prompt': 'You are a item-fair recommender. Please try to ensure that each category of items receives fair recommendations.', 'fairness_type': 'Exposure', 'grounding_model': 'Qwen2-7B', 'group_aggregation_threshold': 10, 'group_id': 'publisher:token', 'group_num': 43, 'group_val': 5, 'history_field': 'history_behaviors', 'history_length': 5, 'item_domain': 'game', 'item_id': 'product_id:token', 'item_num': 1238, 'item_val': 5, 'label_id': 'label:float', 'label_threshold': 10, 'learning_rate': 0.001, 'llm_name': 'Mistral-7B', 'llm_type': 'vllm', 'log_name': 'RAIFPro_ml-100k', 'mmf_eval_ratio': 0.2, 'model': 'RAIFPro', 'rank_model': 'APR', 'reprocess': True, 'sample_num': 200, 'sample_size': 1.0, 'saved_embs_filename': 'qwen_embs.pt', 'stage': 'post-processing', 'store_scores': True, 'task': 'recommendation', 'test_ratio': 0.1, 'text_id': 'title:token', 'timestamp': 'timestamp:float', 'topk': [5, 10, 20], 'use_llm': False, 'user_id': 'user_id:token', 'user_num': 4446, 'user_val': 5, 'valid_ratio': 0.1, 'watch_metric': 'ndcg@5', 'alpha': 0.2, 'candidate': 100, 'eval_batch_size': 128, 'ranking_store_path': '2025-5-30_SASRec_ml-100k', 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy']}\n",
      "loading ranking scores....\n",
      "Running RAIFPro, 0.200000\n",
      "Set parameter Username\n",
      "Set parameter LicenseID to value 2664329\n",
      "Academic license - for non-commercial use only - expires 2026-05-13\n",
      "Gurobi Optimizer version 12.0.2 build v12.0.2rc0 (mac64[arm] - Darwin 21.6.0 21G72)\n",
      "\n",
      "CPU model: Apple M2\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 275 rows, 14587 columns and 27927 nonzeros\n",
      "Model fingerprint: 0xe4b86eae\n",
      "Variable types: 87 continuous, 14500 integer (14500 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e-01, 2e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [5e+00, 2e+02]\n",
      "Found heuristic solution: objective 1203.2740022\n",
      "Presolve added 143 rows and 0 columns\n",
      "Presolve removed 0 rows and 3338 columns\n",
      "Presolve time: 0.03s\n",
      "Presolved: 418 rows, 11249 columns, 21242 nonzeros\n",
      "Variable types: 28 continuous, 11221 integer (11019 binary)\n",
      "\n",
      "Root relaxation: objective 1.415307e+03, 779 iterations, 0.01 seconds (0.02 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 1415.30748    0  131 1203.27400 1415.30748  17.6%     -    0s\n",
      "H    0     0                    1413.1385406 1415.30748  0.15%     -    0s\n",
      "H    0     0                    1413.2181869 1415.30748  0.15%     -    0s\n",
      "H    0     0                    1413.2191158 1415.30748  0.15%     -    0s\n",
      "     0     0 1414.02621    0   98 1413.21912 1414.02621  0.06%     -    0s\n",
      "H    0     0                    1413.2612366 1414.02526  0.05%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 112\n",
      "\n",
      "Explored 1 nodes (873 simplex iterations) in 0.16 seconds (0.20 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 1413.26 1413.22 1413.22 ... 1203.27\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.413261236594e+03, best bound 1.413382990792e+03, gap 0.0086%\n",
      "Running RAIFPro, 0.200000\n",
      "Gurobi Optimizer version 12.0.2 build v12.0.2rc0 (mac64[arm] - Darwin 21.6.0 21G72)\n",
      "\n",
      "CPU model: Apple M2\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 275 rows, 14587 columns and 27927 nonzeros\n",
      "Model fingerprint: 0x13dfc4b7\n",
      "Variable types: 87 continuous, 14500 integer (14500 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e-01, 2e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+01, 4e+02]\n",
      "Found heuristic solution: objective 2408.6146532\n",
      "Presolve added 143 rows and 0 columns\n",
      "Presolve removed 0 rows and 2103 columns\n",
      "Presolve time: 0.03s\n",
      "Presolved: 418 rows, 12484 columns, 22477 nonzeros\n",
      "Found heuristic solution: objective 2683.1489844\n",
      "Variable types: 14 continuous, 12470 integer (12254 binary)\n",
      "Found heuristic solution: objective 2689.1489844\n",
      "\n",
      "Root relaxation: objective 2.813528e+03, 1067 iterations, 0.01 seconds (0.03 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 2813.52826    0  133 2689.14898 2813.52826  4.63%     -    0s\n",
      "H    0     0                    2811.2209021 2813.52826  0.08%     -    0s\n",
      "H    0     0                    2811.5092823 2813.52826  0.07%     -    0s\n",
      "H    0     0                    2811.5584534 2813.52826  0.07%     -    0s\n",
      "H    0     0                    2811.5671816 2813.52826  0.07%     -    0s\n",
      "H    0     0                    2811.5907523 2813.52826  0.07%     -    0s\n",
      "     0     0 2812.68661    0   95 2811.59075 2812.68661  0.04%     -    0s\n",
      "H    0     0                    2811.8502114 2812.68659  0.03%     -    0s\n",
      "H    0     0                    2811.9250625 2812.68659  0.03%     -    0s\n",
      "     0     0          -    0      2811.92506 2812.16957  0.01%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 81\n",
      "\n",
      "Explored 1 nodes (1221 simplex iterations) in 0.21 seconds (0.29 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 10: 2811.93 2811.85 2811.59 ... 2408.61\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.811925062539e+03, best bound 2.812169571172e+03, gap 0.0087%\n",
      "Running RAIFPro, 0.200000\n",
      "Gurobi Optimizer version 12.0.2 build v12.0.2rc0 (mac64[arm] - Darwin 21.6.0 21G72)\n",
      "\n",
      "CPU model: Apple M2\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 275 rows, 14587 columns and 27927 nonzeros\n",
      "Model fingerprint: 0xbe9305cc\n",
      "Variable types: 87 continuous, 14500 integer (14500 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e-01, 2e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e+01, 8e+02]\n",
      "Found heuristic solution: objective 4591.1807407\n",
      "Presolve added 143 rows and 0 columns\n",
      "Presolve removed 0 rows and 587 columns\n",
      "Presolve time: 0.04s\n",
      "Presolved: 418 rows, 14000 columns, 23993 nonzeros\n",
      "Found heuristic solution: objective 5073.8132143\n",
      "Variable types: 30 continuous, 13970 integer (13770 binary)\n",
      "Found heuristic solution: objective 5102.4132143\n",
      "\n",
      "Root relaxation: objective 5.568169e+03, 1084 iterations, 0.02 seconds (0.03 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 5568.16931    0  123 5102.41321 5568.16931  9.13%     -    0s\n",
      "H    0     0                    5566.4644151 5568.16931  0.03%     -    0s\n",
      "H    0     0                    5567.0079529 5568.16931  0.02%     -    0s\n",
      "     0     0          -    0      5567.00795 5567.54403  0.01%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 76\n",
      "  StrongCG: 1\n",
      "\n",
      "Explored 1 nodes (1163 simplex iterations) in 0.19 seconds (0.23 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 5567.01 5566.46 5102.41 ... 4591.18\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 5.567007952905e+03, best bound 5.567544029125e+03, gap 0.0096%\n",
      "{'ndcg@5': np.float64(0.999), 'u_loss@5': np.float64(0.0025), 'MinMaxRatio@5': np.float64(0.03), 'MMF@5': np.float64(0.2138), 'GINI@5': np.float64(0.4694), 'Entropy@5': np.float64(4.581), 'ndcg@10': np.float64(0.9987), 'u_loss@10': np.float64(0.0034), 'MinMaxRatio@10': np.float64(0.03), 'MMF@10': np.float64(0.2138), 'GINI@10': np.float64(0.4685), 'Entropy@10': np.float64(4.5811), 'ndcg@20': np.float64(0.9979), 'u_loss@20': np.float64(0.0056), 'MinMaxRatio@20': np.float64(0.0275), 'MMF@20': np.float64(0.2124), 'GINI@20': np.float64(0.4718), 'Entropy@20': np.float64(4.5769)}\n",
      "result and config dump in recommendation/log/2025-5-30_RAIFPro_ml-100k\n"
     ]
    }
   ],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"steam\" --train_config_file \"postprocessing_new_model.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Evaluation Resultsüìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         0.9990  0.9987  0.9979\n",
      "u_loss       0.0025  0.0034  0.0056\n",
      "MinMaxRatio  0.0300  0.0300  0.0275\n",
      "MMF          0.2138  0.2138  0.2124\n",
      "GINI         0.4694  0.4685  0.4718\n",
      "Entropy      4.5810  4.5811  4.5769\n"
     ]
    }
   ],
   "source": [
    "# evaluation on fairness re-ranking algorithm applied on <base_model>\n",
    "print_evaluation_results(model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 @5     @10     @20\n",
      "ndcg         0.3337  0.3520  0.3584\n",
      "mrr          0.4371  0.4495  0.4538\n",
      "hr           0.3390  0.3823  0.4100\n",
      "mmf          0.0648  0.0724  0.0890\n",
      "gini         0.6563  0.6403  0.6193\n",
      "entropy      2.6317  2.6748  2.7412\n",
      "maxminratio  0.0046  0.0050  0.0223\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
